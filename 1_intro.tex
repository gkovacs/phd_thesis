\msb{In this chapter, and the chapters to follow, each one should start with an introduction that transitions into it and places this chapter into the context of the larger document. Just jumping in with ``novelty wears off over time'' is disorienting!}

\section{Introduction}

Behavior change systems today suffer from declining effectiveness as novelty wears off over time. Typically, behavior change systems utilize a \textit{static} design, which never changes. For example, to manage social media browsing time, the three popular options are to block tempting sites~\cite{stayfocusd}, use a work timer~\cite{focusbooster}, and audit time spent~\cite{khovanskaya2013everybody,rescuetimeapp}. However, static interventions suffer from high attrition and abandonment rates~\cite{collins2014social,eysenbach2005law}, and interventions decline in effectiveness over time~\cite{krebs2010meta}. Habituation eventually drives users to stop paying attention to, or avoid, static interventions---an effect often seen on the web as banner blindness~\cite{benway1998banner}. The end result is that many behavior change systems are tuned out by users, and are unsuccessful at their goals.

If static interventions are tuned out, \textit{rotation} might provide a remedy. Much like a human coach or tutor rotates between different approaches over time, rotation might maintain attention in ways that static interventions cannot. Online behavior change tools could apply similar techniques, for example injecting a visible stopwatch timer into the page on one visit to Facebook, and hiding comments on the next visit.
Techniques that personalize interventions via multi-armed bandits show positive treatment effects, suggesting that the approach may hold promise, but this existing work cannot separate the effects of personalization from the effects of rotation~\cite{rabbi2015automated, lei2017actor, paredes2014poptherapy}. %Techniques that rotate between different interventions, selected randomly or via algorithms such as multi-armed bandits, improve overall effectiveness in health intervention contexts. 
Because rotated interventions continually change the user interface, however, they may frustrate users by violating consistency and a sense of user control, leading to lower effectiveness or higher attrition. %\rak{Has this (that rotating interventions frustrate users) been measured or is this a hypothesis?}

This paper takes up the question: are static or rotated interventions more effective for behavior change? Is it possible to understand the effects of rotation in order to design more effective behavior change systems? We focus specifically on helping users who want to manage their time on social media websites such as Facebook, YouTube, Reddit, and Twitter. We perform a series of field experiments with people who sought out and installed a browser extension that we developed for online behavior change. % \rak{Maybe state why you choose these platforms? Are these sites that people spend a lot of time on? Or are they sites that people usually pick add interventions to?}

Our platform, \textit{HabitLab} (\url{https://habitlab.stanford.edu}) is a Chrome extension that features a number of online productivity interventions to help users reduce their time spent on sites such as Facebook. We released HabitLab publicly on the Chrome web store, where it has attracted over 8,000 daily active users. This user base allows us to observe real-world usage and attrition patterns over time. % real-world realistic,
\msb{Think about how you want to handle these kinds of claims in the thesis. Do you want to say that as of this chapter, it had N users? That for this chapter, you used a dataset up to date Y, which had N users?}

We ran three in-the-wild studies on users who newly installed HabitLab. In Study 1, we compared static interventions to rotation. %Conditions rotated interventions with each new session, or every one, three, five, or seven days. 
We measured effectiveness through time on the user's targeted site, and we measured attrition by tracking when users stopped using the extension.
Results indicate that rotation is a double-edged sword. %Static interventions declined in effectiveness over time, with an estimated 5.9\% daily increase in time spent over our experiment. 
Rotating interventions reduced time spent on sites by $34\%$ per day, but at the cost of nearly doubling attrition levels.  %A follow-up correlational analysis suggests that rotation attrition was less likely when users chose their interventions themselves during installation, suggesting that these users either had a better mental model of the rotation to come, or that user control plays an important role.

% \jingyi{you don't say how long study 1 was for}
Study 2 replicates the first experiment over a longer period of seventy days, and additionally tests whether the number of interventions included in the rotation impacts attrition. The results successfully replicated the original results over a longer 70-day period, and suggested that the larger the set of interventions, the higher the probability of attrition.

% \rak{You said that this number was 50\% in the intro. Make them consistent.}
To investigate the underlying causes of attrition and mitigate the effects of rotation on attrition, we analyzed user feedback and developed a pair of interface techniques to improve the user experience in the presence of rotation, which we deployed in Study 3. The first technique is informational, aiding people's mental models by reminding them that the system may show a different intervention on each visit. The second technique focuses on user control, providing the same information as well as a just-in-time mechanism for people to opt out of each new intervention as they see it. Results indicated that these interventions reduced attrition by over half, so that $80\%$ of new users were still using the system actively after a week.

% \jingyi{``and others'' vague}
In sum, this paper contributes the first comparison of static and rotation intervention strategies in behavior change, a living laboratory system that allows us to deploy this investigation and other field experiments, and interaction design strategies that can help offset increased attrition due to rotation. Its results suggest that people may be more able to control their social media usage more effectively than using today's common techniques such as site blockers. The rest of the paper is organized as follows: we first review studies of behavior change to develop our research question and hypotheses; we then describe our studies and results; we close with reflections and future design directions.







\begin{comment}
Excessive recreational online activity, or ``cyberslacking'' is a major problem. Hence, numerous productivity interventions, such as time trackers and site blockers, have emerged to help users reduce their time online. Existing productivity tools typically always show the same intervention; however, techniques that alternate between different interventions (selected via algorithms such as multi-armed bandits) have been found to improve overall effectiveness in various health intervention contexts. This can be applied to online productivity tools as well -- i.e., hiding the news feed on one visit, injecting a timer into the page on others, etc.

Alternating interventions has typically been done in the context of an adaptive intervention selection algorithm (e.g. multi-armed bandits), and the benefits have been attributed to personalization -- certain interventions are more effective for certain individuals, and by exploring which interventions are most effective for the individual, the algorithm can maximize effectiveness in the long run. 

However, we believe that our understanding of the effects of alternating interventions may be incomplete. Certain health interventions have been found to decline in effectiveness over time, and banner blindness is a well-known phenomenon in online advertising -- if users analogously develop blindness to interventions, could the novelty brought by alternating interventions itself improve effectiveness, by preventing the user from developing intervention blindness? Such effects are important, because declines in intervention effectiveness over time violates a major assumption of multi-armed bandit algorithms -- that the rewards, e.g. the effectiveness of the interventions -- do not change over time. If alternating interventions has a significant positive effect on effectiveness, intervention selection algorithms need to take this into account.

% https://www.sciencedirect.com/science/article/pii/S0091743510002318

Alternatively, alternating interventions can have negative effects -- users may be annoyed and fatigued if they constantly see different interventions, as it may violate expectations of a consistent user experience. Users may also feel a lack of control if the system is choosing which interventions to show them, rather than always showing the same one. If a user particularly dislikes one of the interventions, the user may be exposed to it while the system is exploring the space of interventions, which risks causing user dissatisfaction. These factors may lead to alternating interventions contributing to increased rates of attrition -- which intervention selection algorithms need to take into account.

To answer these questions of in which ways alternating interventions can be beneficial or harmful, we developed a Chrome extension, HabitLab, that features a number of online productivity interventions to help users reduce their time spent on sites like Facebook. We released it publicly on the Chrome web store, and ran a pair of in-the-wild studies on users who newly installed the extension, where we varied how much we were alternating between interventions. Users were unpaid, organic installs from the Chrome web store rather than artificially recruited participants, which allows us to better observe real-world usage and attrition patterns. We found the following results:

\begin{enumerate}
\item Users who are constantly rotating between interventions have higher rates of attrition than users who consistently see the same intervention.
\item Interventions decline in effectiveness over time
\item Interventions are more effective at reducing users' time spent when they are shown alternating between different interventions
\item Users are less likely to attrition if they chose their interventions themselves during the onboarding process
\end{enumerate}

These results led us to hypothesize that the increase in attrition observed due to alternating interventions may be a result of violation of user expectations (due to users having an incorrect mental model), or users feeling a lack of control.

To investigate the underlying cause and mitigate this attrition, we developed a pair of interface techniques shown when a new intervention is seen for the first time. The first interface just repeats information (that users saw during onboarding but may not have read) that the system may show a different intervention on each visit, while the second interface gives users a sense of control by allowing them to opt-out of seeing the intervention in the future. We found that adding these interventions both significantly decrease attrition (fake result).

\end{comment}

%These results led us to propose a new UI technique that retains the benefits of alternating interventions, while reducing the negative effects on attrition. Whenever

%These results lead us to suggest new algorithmic and UI techniques for behavior change interventions that allow us to exploit the benefits of techniques that alternate between interventions, while avoiding the negative effects on attrition.

% Algorithmic techniques we propose to limit attrition caused by alternating between interventions are 1) limit the exploration speed such that the user is not overwhelmed by the rate at which they are seeing new interventions, and 2) model individual interventions' likelihood of causing attrition so that the algorithm is less likely to show interventions which are likely to cause attrition.

% UI techniques we propose to give users a sense of control and reduce violation of expectations caused by the system randomly introducing new interventions are 1) explicitly show users when a new intervention is going to be introduced with an option to opt-out of seeing it, and 2) make the schedule at which new interventions are introduced predictable and known to the user.

% However, online productivity tools operate on a different timescale 

% The traditional narrative on multi-armed bandits has been that they provide benefits via personalization -- certain interventions are more effective for certain individuals, and by exploring which interventions are most effective 



%People spend a lot of time online

%Productivity interventions try to reduce their time online

%There are techniques that make use of an ensemble of interventions, that can outperform just using a single intervention, for example multi-armed bandits

% However we suspect that users develop blindness to interventions over time, hence we wished to investigate whether novelty effects exist and to quantify their magnitude.

% Novelty effects are important, because declines in intervention effectiveness over time violates the assumptions of multi-armed bandit algorithms.

% We investigated this in the context of a system we developed, HabitLab, which is a Chrome extension that features a . It is published on the Chrome store and can be used by anybody, and currently has over 4000 daily active users.

% We found that 

