\section{Discussion}

\msb{Would love to see a page of Discussion in this chapter. Possible topics include: (1) Do you think the same design principles would allow a research system to succeed in other domains? If so, why? If not, why not? (2) What did we try that didn't work, and why? (3) Of the stuff that you did in designing HabitLab, what do you think is the most central novel idea?} \geza{done}

The HabitLab system was built on the premise that experimentation for behavior change can be useful for both end users as well as researchers. Based on our experiences building, growing, and receiving feedback on the system, we have reasons to believe that this premise is true in certain respects, while false in others. Specifically, certain types of experimentation can be well-aligned with end user goals and perhaps even enhance the user experience, but other types of experimentation may be detrimental to the user experience.

We observed that among negative feedback we received for the system, the most common complaint related to experience sampling -- users simply did not like being forced to answer questions, regardless of how minimally intrusive they were. The next most common complaints were necessary artifacts of our experimentation and A/B testing -- some users wanted more control over the frequency of interventions and which interventions were seen at particular times, but we wanted to randomize these as part of our experiments. We also often found that users requested additional customizations that would provide them with more control over interventions, but were reluctant to implement them as they would complicate experimentation by making it more difficult to quantify intervention effectiveness.

We also find that users may be unsatisfied with ``baseline'' or ``control'' conditions in an in-the-wild context. Specifically, some of our experiments necessitated that users occasionally not be shown any intervention on a particular visit, so that we can observe baseline time spent in the absence of an intervention. However, whenever we had such an experiment running, this resulted in a flurry of user complaints that interventions were not working. Even when users were aware of this, or were aware of the rotating nature of interventions, they expressed dissatisfaction that they were not able to see particular interventions each visit. Thus, this is a clear example of researchers' needs for experimentation being in conflict with users' preferences.

That said, several forms of experimentation did not conflict much with user expectations. In contrast to experience sampling, which resulted in both user complaints and increased attrition, we found that adding the same question as a one-time onboarding question did not result in user complaints or increased attrition. Likewise, while we found that the intervention rotation strategy increased attrition and resulted in some user complaints, it also increased intervention effectiveness.

Thus, while we would optimally always be able to find experiments which both satisfy scientific needs as well as preferences of end users, they are often in conflict. In-the-wild experimentation platforms such as HabitLab must sometimes make compromises between these goals. With HabitLab, our approach to navigating this problem has tended to be experimental. Specifically, we A/B test our experiments and constantly monitor user feedback and attrition rates in experimental conditions compared to baselines which we know to have good user satisfaction. If a particular experiment is causing a large increase in attrition, or users are unsatisfied, we attempt to modify the experiment design so we can answer the same research questions with less increase in attrition. For instance, since we found that our original experience sampling prompt design causes an increase in attrition, we A/B tested several different variations to try to find alternative designs which would collect similar data while causing less attrition.

Our general approach to this experimentation-vs-user-experience tradeoff with HabitLab can thus be described as using A/B testing so we can understand the tradeoff we are making, and looking for alternative experiment designs that satisfy our scientific goals with better user experience if they are in conflict. We believe that designers of in-the-wild experiment platforms must always be thinking about user experience, retention, and satisfaction, and cannot sacrifice these for the sake of running an experiment -- after all, an in-the-wild experiment platform's most precious asset is its userbase, and if there are no users, we cannot run any experiments.

