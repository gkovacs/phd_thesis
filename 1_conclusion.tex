\section{Conclusion}

Behavior change intervention effectiveness declines as interventions are repeatedly shown to the user. This decline can be combated by rotating between a stable set of different interventions. Rotating interventions increases attrition, but user interface changes can ameliorate the issue. Taken together, these results suggest opportunities to build behavior change systems that operate more like coaches and tutors: they might explore different strategies to find what works well, and then occasionally rotate to keep things fresh. %Just like coaches must personalize their strategies to each person, these systems can use the feedback to learn what works for each person over time, and reflect that information back to the user for more effective metacognition and agency.

More broadly, there is a large opportunity for behavior change research through big data and crowdsourcing that has been under-explored due to the paucity of large-scale deployments of research systems. Could we predict which interventions will work well for a new user, before they even start using the system? Could we automatically deploy and test modified versions of interventions, to hill-climb our way to more effective ones? Could we enlist an engaged user community to come up with, generate, and test new interventions for the long-tail of behavior change goals that designers had never even thought of? These can be realized with machine learning and crowdsourcing techniques, but there have not been appropriate communities for an in-the-wild deployment. We hope HabitLab will provide such a platform to realize this vision of community-driven behavior change research  in the wild. % through self-experimentation.


%We suspect the underlying cause is users not being used to having interventions change, or feeling a lack of control. To combat this increase in attrition, we developed a pair of UI notifications presented whenever a new intervention is presented. We found that they significantly reduced attrition rates.


