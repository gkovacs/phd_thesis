\chapter{Discussion}

%\msb{Same comment as in related work --- discussion relevant to a single chapter's results should go into that chapter. This chapter should be reserved for higher-level reflections about the broader mission of the thesis.} \geza{done}

%\msb{Removing the rotation-specific stuff, this Discussion chapter is too anemic. See Joy's dissertation as an example of what I'd like to see: \url{https://searchworks.stanford.edu/view/11919674}. This is your chance to reflect more broadly on what you think the active ingredient of your success was, and what kind of advice you'd pass on to someone else who wants to create something like HabitLab. Also, what are the limitations on the generality of HabitLab and its techniques? And also, what would you envision as what an ideal behavior change system might look like given everything you've learned from HabitLab and from the studies you've run?} \geza{done}

This thesis advances a vision of large-scale, in-the-wild experimentation which can simultaneously gather insights about behavior change across a large body of volunteer users, while at the same time providing users with a useful behavior change tool that they will voluntary use. These insights can then be used to create more effective behavior change system. This vision fits into a larger design space of in-the-wild research that attempts to find an intersection between the needs and incentives of end users, and the scientific needs of researchers. In this chapter will discuss our design principles for in-the-wild experimentation, our visions for the future of in-the-wild behavior change experimentation systems, as well as limitations of our general approach.

\section{Design Principles for In-the-wild Experimentation}

Our design principles with HabitLab focused on maximizing user retention rates, which we believe helped us with growth. We did so by giving users control of their interventions, providing visually appealing and unobtrusive interventions, and avoiding intrusive surveys and experience sampling as much as possible.

We believe the key to our success with HabitLab was our approach of prioritizing user experience, retention, and growth. When conflicts between user experience and other research goals emerged, we would quantify the cost by conducting A/B tests to see the effects on retention, and opted for experiment designs which would give us the most useful data, with the least cost in retention. Our choice of studies to run, and research questions to pursue, was also influenced by this tradeoff -- we chose research questions which would require minimal degradation in the user experience, and opted to not pursue those that would require extensive experience sampling and result in user dissatisfaction.

We believe there are many parallels between building in-the-wild experimentation systems, and monetizing commercial products, and researchers in this space can potentially learn much from the world of monetization. Specifically, from a user growth perspective in a commercial product, it is often best to prioritize user growth initially, and prioritize monetization later -- and we see this play out in the business strategies of many consumer-oriented startups. Likewise, when building HabitLab, we focused our early efforts on many features that were requested by users and helped retention, but were unrelated to any research questions -- these early investments towards user growth assisted in ensuring we later had a sufficiently large active userbase to run our experiments. That said, with commercial products, the product and monetization strategy must be co-designed, so that the monetization strategy is aligned with user goals, and does not drive away users. Likewise, in the context of in-the-wild experimentation systems, research questions and the system must likewise be co-designed, so that the experiments do not drive away users.

\section{Visions for the Future of Behavior Change Systems}

What would an ideal behavior change system look like? Based on our studies with HabitLab, we believe it would be minimally intrusive, require minimal configuration, and yet be responsive to user preferences. It would include a variety of interventions, but the specifics of the interventions the user sees would depend highly on user preferences, contexts, and their goals.

A major advantage we had with the domain we chose -- online behavior change -- is that we could push our interventions automatically to users on every visit, without requiring any interaction on their part. This may be considerably more difficult to realize in other behavior change domains, where it may be more difficult to sense when the targeted activity is taking place. For instance, dieting apps unable to detect when the user is about to eat may require the user to explicitly open the app to indicate when and what they are eating, so different design strategies -- such as unprompted push notifications that some might consider to be intrusive -- may be needed for other behavior change domains.

That said, we envision that with the increasing ubiquity of sensors, augmented reality, and wearable devices, this objective of sensing behaviors in-the-wild and responding to them with interventions will become increasingly easy to realize. Once we are better able to sense the world, and can deploy interventions to the real world beyond our phones and browsers, a number of common behavior change goals -- such as sleep, exercise, health, and dieting -- become amenable to behavior change experimentation. We can envision, for example, using augmented reality to make our unhealthy food seem less appealing, and making our healthy food seem more appealing. Such an experiment can be deployed across a variety of users throughout the world, applying various filters to our food images as seen through our AR devices, and measuring the outcomes to determine the most effective filters. Another example might be determining what interventions are most effective to get a user to burn the most calories -- some users may be motivated by social proof and competition from peers, others may be motivated by financial rewards, etc. We believe that compared to past behavior change studies, these new modalities of sensing outcomes and deploying interventions will open a the gates to much wider and potentially more effective interventions, as well as lowering the barrier to participation, enabling experimentation to be done at large scales.

% For the first study on rotation, we believe that novelty is an underlying mechanism for the improvement in effectiveness we observed when interventions are rotated. This leads us to speculate: would it be a effective and practical strategy to scale up the number of interventions, so that we are rotating between interventions from a huge pool of hundreds of interventions? Or do the improvements in effectiveness that we can expect from rotating increasing numbers of interventions have limits? We speculate that increasing the number of interventions in rotation will have high initial benefits for the first few additional interventions, but will have declining benefits as more interventions are added, as the probability of repeatedly seeing a recently-seen intervention grows increasingly small.

% The findings of our second study about time redistribution have a positive tone -- we did not observe negative side effects of productivity interventions, which would have been predicted if users were using their devices to replenish willpower when exhausted. Perhaps one speculative explanation is that in the context of device usage, diminished willpower results in the user opening a site or visiting an app, but actually spending time on the site or app does not replenish this willpower -- only the initial act of opening the site or app does. This may have interesting implications if it is true in other domains as well -- for instance, if the user is on a diet and has a craving for doughnuts, would an intervention preventing them from eating a doughnut also suppress cravings for other fattening foods as well? If they give in to the craving, would stopping them after the first bite leave their craving equally satisfied as if we let them eat the whole doughnut?

\section{Limitations}

Given we conducted these studies in the context of reducing time spent online and on phones, results may not necessarily generalize to other behavior change domains. That said, with our increasing ability to sense our environment via sensors in our phones, smartwatches, and IoT devices, many of the paradigms we used in HabitLab can be applied to other domains as well. For instance, in the fitness domain, if we can sense users' physical activity levels via a smartwatch, we can experiment with various interventions that prompt users to exercise by playing audio messages or sending notifications. This hypothetical in-the-wild experimentation platform for fitness could potentially work analogously to HabitLab, running studies to find intervention strategies that work well to increase physical activity levels. The increasing ubiquity of sensors in the physical world make this paradigm of in-the-wild behavior change increasingly realistic and possible in domains outside online behavior change.

With regards to the general modality of in-the-wild behavior change experimentation with voluntary users, a major limitation is that we can only study behaviors that users truly want to change. For instance, if our behavior change objective is to cause users to gamble more, or spend more money shopping, users may likely be less willing to explicitly opt into voluntarily participating in such an experiment. Even if the behavior change objective is one that may appeal to users -- for example, losing weight -- certain types of interventions may be difficult to run accurate experiments with an in-the-wild, voluntary population. For example, if the intervention is to force the user to fast without any ability to opt-out, the user may end up interfering with sensors so they cannot detect them eating. Likewise, we suspect that if HabitLab were to start blocking sites entirely, users may be tempted to bypass the system by using another device or browser. Thus, we believe that when the incentives of the researchers and users do not align well, other means of experimentation -- such as using compensated users in controlled lab settings, or forcing them to participate as the precondition for using a system -- will still be more suitable.
