\chapter{Discussion}

%\msb{Same comment as in related work --- discussion relevant to a single chapter's results should go into that chapter. This chapter should be reserved for higher-level reflections about the broader mission of the thesis.} \geza{done}

%\msb{Removing the rotation-specific stuff, this Discussion chapter is too anemic. See Joy's dissertation as an example of what I'd like to see: \url{https://searchworks.stanford.edu/view/11919674}. This is your chance to reflect more broadly on what you think the active ingredient of your success was, and what kind of advice you'd pass on to someone else who wants to create something like HabitLab. Also, what are the limitations on the generality of HabitLab and its techniques? And also, what would you envision as what an ideal behavior change system might look like given everything you've learned from HabitLab and from the studies you've run?} \geza{done}

Our design principles with HabitLab focused on maximizing user retention rates, which we believe helped us with growth. We did so by giving users control of their interventions, providing visually appealing and unobtrusive interventions, and avoiding intrusive surveys and experience sampling as much as possible.

We believe the key to our success with HabitLab was our approach of prioritizing user experience, retention, and growth. When conflicts between user experience and other research goals emerged, we would quantify the cost by conducting A/B tests to see the effects on retention, and opted for experiment designs which would give us the most useful data, with the least cost in retention. Our choice of studies to run, and research questions to pursue, was also influenced by this tradeoff -- we chose research questions which would require minimal degradation in the user experience, and opted to not pursue those that would require extensive experience sampling and result in user dissatisfaction.

We believe there are many parallels between building in-the-wild experimentation systems, and monetizing commercial products, and researchers in this space can potentially learn much from the world of monetization. Specifically, from a user growth perspective in a commercial product, it is often best to prioritize user growth initially, and prioritize monetization later -- and we see this play out in the business strategies of many consumer-oriented startups. Likewise, when building HabitLab, we focused our early efforts on many features that were requested by users and helped retention, but were unrelated to any research questions -- these early investments towards user growth assisted in ensuring we later had a sufficiently large active userbase to run our experiments. That said, with commercial products, the product and monetization strategy must be co-designed, so that the monetization strategy is aligned with user goals, and does not drive away users. Likewise, in the context of in-the-wild experimentation systems, research questions and the system must likewise be co-designed, so that the experiments do not drive away users.

What would an ideal behavior change system look like? Based on our studies with HabitLab, we believe it would be minimally intrusive, require minimal configuration, and yet be responsive to user preferences. It would include a variety of interventions, but the specifics of the interventions the user sees would depend highly on user preferences, contexts, and their goals.

A major advantage we had with the domain we chose -- online behavior change -- is that we could push our interventions automatically to users on every visit, without requiring any interaction on their part. This may be considerably more difficult to realize in other behavior change domains, where it may be more difficult to sense when the targeted activity is taking place. For instance, dieting apps unable to detect when the user is about to eat may require the user to explicitly open the app to indicate when and what they are eating, so different design strategies -- such as unprompted push notifications that some might consider to be intrusive -- may be needed for other behavior change domains.

% For the first study on rotation, we believe that novelty is an underlying mechanism for the improvement in effectiveness we observed when interventions are rotated. This leads us to speculate: would it be a effective and practical strategy to scale up the number of interventions, so that we are rotating between interventions from a huge pool of hundreds of interventions? Or do the improvements in effectiveness that we can expect from rotating increasing numbers of interventions have limits? We speculate that increasing the number of interventions in rotation will have high initial benefits for the first few additional interventions, but will have declining benefits as more interventions are added, as the probability of repeatedly seeing a recently-seen intervention grows increasingly small.

% The findings of our second study about time redistribution have a positive tone -- we did not observe negative side effects of productivity interventions, which would have been predicted if users were using their devices to replenish willpower when exhausted. Perhaps one speculative explanation is that in the context of device usage, diminished willpower results in the user opening a site or visiting an app, but actually spending time on the site or app does not replenish this willpower -- only the initial act of opening the site or app does. This may have interesting implications if it is true in other domains as well -- for instance, if the user is on a diet and has a craving for doughnuts, would an intervention preventing them from eating a doughnut also suppress cravings for other fattening foods as well? If they give in to the craving, would stopping them after the first bite leave their craving equally satisfied as if we let them eat the whole doughnut?

Given we conducted these studies in the context of reducing time spent online and on phones, results may not necessarily generalize to other behavior change domains. That said, with our increasing ability to sense our environment via sensors in our phones, smartwatches, and IoT devices, many of the paradigms we used in HabitLab can be applied to other domains as well. For instance, in the fitness domain, if we can sense users' physical activity levels via a smartwatch, we can experiment with various interventions that prompt users to exercise by playing audio messages or sending notifications. This hypothetical in-the-wild experimentation platform for fitness could potentially work analogously to HabitLab, running studies to find intervention strategies that work well to increase physical activity levels. The increasing ubiquity of sensors in the physical world make this paradigm of in-the-wild behavior change increasingly realistic and possible in domains outside online behavior change.

