\chapter{Discussion and Conclusion}

In this thesis we have proposed a paradigm of in-the-wild experimentation to gain insights about behavior change, and have created a platform, HabitLab, to realize this vision in the context of helping users reduce their time online and on their phones. We also conducted a set of studies on HabitLab which illustrate that we can make novel findings with the system.

Our design principles with HabitLab focused on maximizing user retention rates, which we believe helped us with growth. We did so by giving users control of their interventions, providing visually appealing and unobtrusive interventions, and avoiding intrusive surveys and experience sampling as much as possible.

A major advantage we had with the domain we chose -- online behavior change -- is that we could push our interventions automatically to users on every visit, without requiring any interaction on their part. This may be considerably more difficult to realize in other behavior change domains, where it may be more difficult to sense when the targeted activity is taking place. For instance, dieting apps unable to detect when the user is about to eat may require the user to explicitly open the app to indicate when and what they are eating, so different design strategies -- such as unprompted push notifications that some might consider to be intrusive -- may be needed for other behavior change domains.

The first set of studies we ran with HabitLab investigated whether interventions decline in effectiveness over time. We found that interventions decline in effectiveness if the same intervention is repeatedly shown, and that a strategy of rotating between different interventions can help improve the effectiveness. While this comes at the cost of increased attrition, most likely due to users having incorrect mental models, we can reduce this attrition via a simple design shown when a new intervention is introduced.

We believe that novelty is an underlying mechanism for the improvement in effectiveness we observed when interventions are rotated. This leads us to speculate: would it be a effective and practical strategy to scale up the number of interventions, so that we are rotating between interventions from a huge pool of hundreds of interventions? Or do the improvements in effectiveness that we can expect from rotating increasing numbers of interventions have limits? We speculate that increasing the number of interventions in rotation will have high initial benefits for the first few additional interventions, but will have declining benefits as more interventions are added, as the probability of repeatedly seeing a recently-seen intervention grows increasingly small.

The second set of studies investigated whether interventions that help save time on one site, app, or device influence time spent elsewhere. We found that on the browser, reducing time on one site has a beneficial side effect of reducing time elsewhere. We believe this is due to reducing time on aggregator sites that drive traffic to other sites. On phones, however, we did not observe any side effects of reducing time on one app on other apps. We also did not observe any cross-device effects.

The findings of this study have a positive tone -- we did not observe negative side effects of productivity interventions, which would have been predicted if users were using their devices to replenish willpower when exhausted. Perhaps one speculative explanation is that in the context of device usage, diminished willpower results in the user opening a site or visiting an app, but actually spending time on the site or app does not replenish this willpower -- only the initial act of opening the site or app does. This may have interesting implications if it is true in other domains as well -- for instance, if the user is on a diet and has a craving for doughnuts, would an intervention preventing them from eating a doughnut also suppress cravings for other fattening foods as well? If they give in to the craving, would stopping them after the first bite leave their craving equally satisfied as if we let them eat the whole doughnut?

Given we conducted these studies in the context of reducing time spent online and on phones, results may not necessarily generalize to other behavior change domains. That said, with our increasing ability to sense our environment via sensors in our phones, smartwatches, and IoT devices, many of the paradigms we used in HabitLab can be applied to other domains as well. For instance, in the fitness domain, if we can sense users' physical activity levels via a smartwatch, we can experiment with various interventions that prompt users to exercise by playing audio messages or sending notifications. This hypothetical in-the-wild experimentation platform for fitness could potentially work analogously to HabitLab, running studies to find intervention strategies that work well to increase physical activity levels. The increasing ubiquity of sensors in the physical world make this paradigm of in-the-wild behavior change increasingly realistic and possible in domains outside online behavior change.

There is a large opportunity for behavior change research through big data and crowdsourcing that has been under-explored due to the paucity of large-scale deployments of research systems. Could we predict which interventions will work well for a new user, before they even start using the system? Could we automatically deploy and test modified versions of interventions, to hill-climb our way to more effective ones? Could we enlist an engaged user community to come up with, generate, and test new interventions for the long-tail of behavior change goals that designers had never even thought of? These can be realized with machine learning and crowdsourcing techniques, but there have not been appropriate communities for an in-the-wild deployment. We hope HabitLab will provide such a platform to realize this vision of community-driven behavior change research  in the wild. % through self-experimentation.
